# Project Dependencies Overview

## âœ… Installed & In Use

### 1. HTML Parsing & Data Extraction

#### âœ… **cheerio** (`^1.1.2`)
- **Used for**: Fast, jQuery-like HTML parsing
- **Location**: `lib/scraper/structured-data.ts`, `pages/api/fetch-product-simple.ts`
- **Purpose**: Extract structured data (JSON-LD), meta tags, and product information
- **Why**: Lightweight, server-side DOM manipulation without browser overhead

#### âœ… **jsdom** (`^27.1.0`)
- **Used for**: HTML parsing and DOM manipulation
- **Location**: `lib/scraper/playwright-scraper.ts`, `lib/scraper/scrape-and-save.ts`
- **Purpose**: Parse HTML from Playwright and extract product data
- **Why**: More robust than regex, handles complex HTML structures

### 2. Browser Automation

#### âœ… **playwright** (`^1.40.0`)
- **Used for**: Headless browser automation
- **Location**: All scraper files
- **Purpose**: Navigate dynamic sites, handle JavaScript rendering
- **Why**: Industry standard, reliable, cross-browser support

#### âœ… **playwright-extra** (`^4.3.6`)
- **Used for**: Plugin system for Playwright
- **Location**: `lib/scraper/playwright-scraper.ts`, `lib/scraper/scrape-and-save.ts`
- **Purpose**: Enable stealth plugins to bypass bot detection
- **Why**: Extends Playwright with additional functionality

### 3. HTTP Requests & Fetching

#### âœ… **node-fetch** (`^2.7.0`)
- **Used for**: HTTP requests (fetch API for Node.js)
- **Location**: `lib/scraper/google-cache.ts`, `lib/scraper/scrape-and-save.ts`
- **Purpose**: Fetch HTML from URLs without browser overhead
- **Why**: Lightweight alternative to axios, built-in fetch API compatibility

### 4. Reliability & Concurrency

#### âœ… **p-retry** (`^7.1.0`) - NEW
- **Purpose**: Automatically retry failed requests
- **Usage**: Can be added to scraping functions for automatic retries
- **Example**:
```typescript
import pRetry from 'p-retry';

const result = await pRetry(
  () => scrapeProduct(url),
  { retries: 3, onFailedAttempt: error => console.log(`Attempt ${error.attemptNumber} failed`)}
);
```

#### âœ… **p-queue** (`^9.0.0`) - NEW
- **Purpose**: Control concurrency and rate limiting
- **Usage**: Queue multiple scraping requests to avoid rate limits
- **Example**:
```typescript
import PQueue from 'p-queue';

const queue = new PQueue({ concurrency: 2, interval: 1000 });
await queue.add(() => scrapeProduct(url1));
await queue.add(() => scrapeProduct(url2));
```

### 5. Metadata Extraction

#### âœ… **metascraper** (`^5.38.4`) + plugins
- **Used for**: Extract metadata from web pages
- **Location**: `lib/scraper/static-scraper.ts`
- **Plugins**: description, image, title, url
- **Purpose**: Fallback metadata extraction when structured data fails

### 6. Database & Backend

#### âœ… **@supabase/supabase-js** (`^2.38.0`)
- **Used for**: Supabase database client
- **Location**: `lib/scraper/scrape-and-save.ts`, `pages/api/fetch-product.ts`
- **Purpose**: Save scraped products to Supabase database

### 7. Frontend Framework

#### âœ… **next** (`^14.0.0`)
- **Used for**: React framework with API routes
- **Location**: Entire project
- **Purpose**: Full-stack application (frontend + API routes)
- **Note**: Includes built-in support for:
  - API routes (no need for Express)
  - CORS handling (no need for cors package)
  - Body parsing (no need for body-parser)
  - Environment variables (no need for dotenv)

#### âœ… **react** + **react-dom** (`^18.2.0`)
- **Used for**: UI components
- **Location**: `components/`, `pages/`

#### âœ… **framer-motion** (`^12.23.24`)
- **Used for**: Animations and motion
- **Location**: All component files

### 8. Styling

#### âœ… **tailwindcss** (`^3.3.6`)
- **Used for**: Utility-first CSS
- **Location**: `tailwind.config.js`, `styles/globals.css`

## âŒ Not Needed (Included in Next.js)

### **express** âŒ
- **Why not needed**: Next.js has built-in API routes (`pages/api/`)
- **Alternative**: Use `pages/api/your-endpoint.ts`

### **cors** âŒ
- **Why not needed**: Next.js API routes handle CORS automatically
- **Alternative**: Configure in `next.config.js` if needed

### **body-parser** âŒ
- **Why not needed**: Next.js API routes parse JSON automatically
- **Alternative**: Just use `req.body` directly

### **dotenv** âŒ
- **Why not needed**: Next.js automatically loads `.env.local` files
- **Alternative**: Just create `.env.local` and use `process.env.VAR_NAME`

### **axios** âŒ
- **Why not needed**: We have `node-fetch` which is sufficient
- **Alternative**: Use `fetch()` or `node-fetch` (already installed)

## ğŸš€ Usage Examples

### Using p-retry for Automatic Retries

```typescript
// lib/scraper/scrape-with-retry.ts
import pRetry from 'p-retry';
import { scrapeAndSave } from './scrape-and-save';

export async function scrapeWithRetry(url: string, maxRetries = 3) {
  return pRetry(
    async () => {
      const result = await scrapeAndSave(url);
      if (!result.title || result.title === 'Unknown Item') {
        throw new Error('Failed to extract product data');
      }
      return result;
    },
    {
      retries: maxRetries,
      onFailedAttempt: (error) => {
        console.log(`Attempt ${error.attemptNumber} failed: ${error.message}`);
      },
    }
  );
}
```

### Using p-queue for Rate Limiting

```typescript
// lib/scraper/batch-scraper.ts
import PQueue from 'p-queue';
import { scrapeAndSave } from './scrape-and-save';

export async function scrapeBatch(urls: string[], concurrency = 2) {
  const queue = new PQueue({ 
    concurrency,
    interval: 1000, // 1 second between batches
    intervalCap: 1  // 1 request per interval
  });

  const results = await Promise.all(
    urls.map(url => 
      queue.add(async () => {
        console.log(`Scraping ${url}...`);
        return await scrapeAndSave(url);
      })
    )
  );

  return results;
}
```

## ğŸ“ Current File Structure

```
wist/
â”œâ”€â”€ lib/
â”‚   â””â”€â”€ scraper/
â”‚       â”œâ”€â”€ scrape-and-save.ts      â† Main scraper with retry logic
â”‚       â”œâ”€â”€ playwright-scraper.ts   â† Playwright + platform-specific
â”‚       â”œâ”€â”€ static-scraper.ts       â† Static HTML scraping
â”‚       â”œâ”€â”€ structured-data.ts      â† Cheerio extraction
â”‚       â”œâ”€â”€ google-cache.ts         â† Google cache extraction
â”‚       â””â”€â”€ utils.ts                â† Utility functions
â”‚
â”œâ”€â”€ pages/
â”‚   â””â”€â”€ api/
â”‚       â””â”€â”€ fetch-product.ts        â† API endpoint
â”‚
â”œâ”€â”€ components/                      â† React components
â”œâ”€â”€ styles/                         â† Global styles
â”œâ”€â”€ .env.local                      â† Environment variables (gitignored)
â”œâ”€â”€ package.json
â””â”€â”€ next.config.js
```

## ğŸ¯ Summary

âœ… **Core Scraping**: Playwright + Cheerio + JSDOM  
âœ… **Reliability**: p-retry + p-queue (NEW)  
âœ… **Data Storage**: Supabase  
âœ… **Frontend**: Next.js + React + Framer Motion  
âœ… **Styling**: Tailwind CSS  

**All essential dependencies are installed and ready to use!**

